{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44babcad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MEMORY FOOTPRINT EVALUATION - TWO-WHEELER MODEL (BSA)\n",
    "# Metrics: GPU Memory, Model Size, Parameter Count, Memory Breakdown\n",
    "# ==============================================================================\n",
    "\n",
    "# ========== CELL 1: Install Packages ==========\n",
    "!pip install -q accelerate bitsandbytes peft transformers\n",
    "\n",
    "# Note: After running Cell 1, restart runtime then run cells 2-11\n",
    "\n",
    "\n",
    "# ========== CELL 2: Import Libraries ==========\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "# ========== CELL 3: Memory Utility Functions ==========\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        return {\n",
    "            'allocated_gb': allocated,\n",
    "            'reserved_gb': reserved,\n",
    "            'max_allocated_gb': max_allocated,\n",
    "            'total_gpu_gb': total\n",
    "        }\n",
    "    return {'allocated_gb': 0, 'reserved_gb': 0, 'max_allocated_gb': 0, 'total_gpu_gb': 0}\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in memory\"\"\"\n",
    "    param_size = 0\n",
    "    buffer_size = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    total_size = param_size + buffer_size\n",
    "    return {\n",
    "        'param_size_mb': param_size / (1024**2),\n",
    "        'buffer_size_mb': buffer_size / (1024**2),\n",
    "        'total_size_mb': total_size / (1024**2),\n",
    "        'total_size_gb': total_size / (1024**3)\n",
    "    }\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable and total parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {\n",
    "        'total_params': total_params,\n",
    "        'trainable_params': trainable_params,\n",
    "        'total_params_millions': total_params / 1e6,\n",
    "        'trainable_params_millions': trainable_params / 1e6\n",
    "    }\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "\n",
    "# ========== CELL 4: Clear Memory and Get Baseline ==========\n",
    "clear_memory()\n",
    "baseline_memory = get_gpu_memory()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MEMORY FOOTPRINT MEASUREMENT - TWO-WHEELER MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBaseline GPU Memory: {baseline_memory['allocated_gb']:.4f} GB\")\n",
    "print(f\"Total GPU Memory: {baseline_memory['total_gpu_gb']:.2f} GB\\n\")\n",
    "\n",
    "\n",
    "# ========== CELL 5: Configure Model ==========\n",
    "adapter_name = \"Prithwiraj731/Gemma2-2b_Two-Wheeler\"\n",
    "base_model_name = \"google/gemma-2-2b\"\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Adapter: {adapter_name}\")\n",
    "print(f\"  Base Model: {base_model_name}\\n\")\n",
    "\n",
    "\n",
    "# ========== CELL 6: Load Tokenizer ==========\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_name)\n",
    "after_tokenizer = get_gpu_memory()\n",
    "print(f\"After tokenizer: {after_tokenizer['allocated_gb']:.4f} GB\\n\")\n",
    "\n",
    "\n",
    "# ========== CELL 7: Load Base Model ==========\n",
    "print(\"Loading base model with 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "after_base = get_gpu_memory()\n",
    "base_model_size = get_model_size(base_model)\n",
    "base_params = count_parameters(base_model)\n",
    "\n",
    "print(f\"After base model: {after_base['allocated_gb']:.4f} GB\")\n",
    "print(f\"Base model size: {base_model_size['total_size_mb']:.2f} MB\")\n",
    "print(f\"Base parameters: {base_params['total_params_millions']:.2f} M\\n\")\n",
    "\n",
    "\n",
    "# ========== CELL 8: Load LoRA Adapter ==========\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_name)\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "after_adapter = get_gpu_memory()\n",
    "full_model_size = get_model_size(model)\n",
    "full_params = count_parameters(model)\n",
    "\n",
    "print(f\"After adapter: {after_adapter['allocated_gb']:.4f} GB\")\n",
    "print(f\"Full model size: {full_model_size['total_size_mb']:.2f} MB\")\n",
    "print(f\"Trainable parameters: {full_params['trainable_params_millions']:.2f} M\\n\")\n",
    "\n",
    "\n",
    "# ========== CELL 9: Run Inference to Measure Peak Memory ==========\n",
    "print(\"Running inference to measure peak memory...\")\n",
    "test_prompt = \"What is the recommended lubrication for the engine?\\n\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "after_inference = get_gpu_memory()\n",
    "print(f\"After inference: {after_inference['allocated_gb']:.4f} GB\")\n",
    "print(f\"Peak memory: {after_inference['max_allocated_gb']:.4f} GB\\n\")\n",
    "\n",
    "\n",
    "# ========== CELL 10: Display Summary Results ==========\n",
    "print(\"=\"*70)\n",
    "print(\"TWO-WHEELER MODEL - MEMORY FOOTPRINT RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nGPU Memory Usage:\")\n",
    "print(f\"  Baseline:              {baseline_memory['allocated_gb']:.4f} GB\")\n",
    "print(f\"  After Tokenizer:       {after_tokenizer['allocated_gb']:.4f} GB\")\n",
    "print(f\"  After Base Model:      {after_base['allocated_gb']:.4f} GB\")\n",
    "print(f\"  After LoRA Adapter:    {after_adapter['allocated_gb']:.4f} GB\")\n",
    "print(f\"  After Inference:       {after_inference['allocated_gb']:.4f} GB\")\n",
    "print(f\"  Peak Memory:           {after_inference['max_allocated_gb']:.4f} GB\")\n",
    "print(f\"  Reserved Memory:       {after_inference['reserved_gb']:.4f} GB\")\n",
    "print(f\"  Total GPU Capacity:    {baseline_memory['total_gpu_gb']:.2f} GB\")\n",
    "\n",
    "print(f\"\\nModel Size:\")\n",
    "print(f\"  Model in Memory:       {full_model_size['total_size_mb']:.2f} MB ({full_model_size['total_size_gb']:.4f} GB)\")\n",
    "print(f\"  Parameters:            {full_model_size['param_size_mb']:.2f} MB\")\n",
    "print(f\"  Buffers:               {full_model_size['buffer_size_mb']:.2f} MB\")\n",
    "\n",
    "print(f\"\\nParameter Count:\")\n",
    "print(f\"  Total Parameters:      {full_params['total_params_millions']:.2f} M ({full_params['total_params']:,})\")\n",
    "print(f\"  Trainable (LoRA):      {full_params['trainable_params_millions']:.2f} M ({full_params['trainable_params']:,})\")\n",
    "print(f\"  Frozen:                {(full_params['total_params_millions'] - full_params['trainable_params_millions']):.2f} M\")\n",
    "print(f\"  Trainable Ratio:       {(full_params['trainable_params'] / full_params['total_params'] * 100):.2f}%\")\n",
    "\n",
    "tokenizer_overhead = after_tokenizer['allocated_gb'] - baseline_memory['allocated_gb']\n",
    "base_overhead = after_base['allocated_gb'] - after_tokenizer['allocated_gb']\n",
    "adapter_overhead = after_adapter['allocated_gb'] - after_base['allocated_gb']\n",
    "inference_overhead = after_inference['max_allocated_gb'] - after_adapter['allocated_gb']\n",
    "\n",
    "print(f\"\\nMemory Breakdown:\")\n",
    "print(f\"  Tokenizer Overhead:    {tokenizer_overhead:.4f} GB ({tokenizer_overhead*1024:.2f} MB)\")\n",
    "print(f\"  Base Model Memory:     {base_overhead:.4f} GB ({base_overhead*1024:.2f} MB)\")\n",
    "print(f\"  LoRA Adapter Overhead: {adapter_overhead:.4f} GB ({adapter_overhead*1024:.2f} MB)\")\n",
    "print(f\"  Inference Overhead:    {inference_overhead:.4f} GB ({inference_overhead*1024:.2f} MB)\")\n",
    "\n",
    "gpu_utilization = (after_inference['max_allocated_gb'] / baseline_memory['total_gpu_gb']) * 100\n",
    "print(f\"\\nGPU Utilization:       {gpu_utilization:.2f}%\")\n",
    "\n",
    "\n",
    "# ========== CELL 11: Memory Usage DataFrame ==========\n",
    "memory_df = pd.DataFrame({\n",
    "    'Stage': ['Baseline', 'After Tokenizer', 'After Base Model', 'After LoRA', 'After Inference', 'Peak'],\n",
    "    'Allocated (GB)': [\n",
    "        f\"{baseline_memory['allocated_gb']:.4f}\",\n",
    "        f\"{after_tokenizer['allocated_gb']:.4f}\",\n",
    "        f\"{after_base['allocated_gb']:.4f}\",\n",
    "        f\"{after_adapter['allocated_gb']:.4f}\",\n",
    "        f\"{after_inference['allocated_gb']:.4f}\",\n",
    "        f\"{after_inference['max_allocated_gb']:.4f}\"\n",
    "    ],\n",
    "    'Delta (MB)': [\n",
    "        \"0.00\",\n",
    "        f\"{tokenizer_overhead*1024:.2f}\",\n",
    "        f\"{base_overhead*1024:.2f}\",\n",
    "        f\"{adapter_overhead*1024:.2f}\",\n",
    "        f\"{(after_inference['allocated_gb'] - after_adapter['allocated_gb'])*1024:.2f}\",\n",
    "        f\"{inference_overhead*1024:.2f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED MEMORY USAGE TABLE\")\n",
    "print(\"=\"*70)\n",
    "display(memory_df)\n",
    "\n",
    "\n",
    "# ========== CELL 12: Save Results (Optional) ==========\n",
    "# Uncomment to save and download results\n",
    "\n",
    "# memory_df.to_csv('memory_footprint_2wheeler_results.csv', index=False)\n",
    "# print(\"\\nResults saved to 'memory_footprint_2wheeler_results.csv'\")\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('memory_footprint_2wheeler_results.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
