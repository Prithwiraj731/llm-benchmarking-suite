{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27889525",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# IMPROVED BERT SCORE EVALUATION FOR GOOGLE COLAB\n",
    "# Fixed Version with Multiple Prompt Format Options\n",
    "# ===================================================================\n",
    "\n",
    "# ========== CELL 1: Install Packages ==========\n",
    "!pip install -q bert-score sentencepiece accelerate bitsandbytes peft transformers\n",
    "\n",
    "# Note: After running Cell 1, restart runtime: Runtime ‚Üí Restart runtime\n",
    "# Then run cells 2-10 (skip cell 1 after restart)\n",
    "\n",
    "\n",
    "# ========== CELL 2: Authentication (if using gated models) ==========\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option A: Manual login (you'll paste token when prompted)\n",
    "login()\n",
    "\n",
    "# Option B: Use Colab Secrets (recommended)\n",
    "# Uncomment below if you set up HF_TOKEN in Colab secrets\n",
    "# from google.colab import userdata\n",
    "# hf_token = userdata.get('HF_TOKEN')\n",
    "# login(token=hf_token)\n",
    "\n",
    "\n",
    "# ========== CELL 3: Import Libraries ==========\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# ========== CELL 4: Define Your Dataset ==========\n",
    "# CUSTOMIZE THIS: Replace with your own test questions and expected answers\n",
    "dataset = [\n",
    "    {\n",
    "        \"question\": \"What is the recommended lubrication for the engine of the BSA D14/4 Bantam Supreme motorcycle?\", \n",
    "        \"answer\": \"Engine lubrication: BSA recommends using a mixture of 10W-30 oil, with a minimum of 10W-40 oil, for the engine of the BSA D14/4 Bantam Supreme motorcycle.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Where should an inexperienced owner consult for assistance with major repair work?\", \n",
    "        \"answer\": \"His B.S.A. dealer\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the recommended procedure for claiming assistance under the B.S.A. guarantee?\", \n",
    "        \"answer\": \"Claim assistance through the dealer from whom the motorcycle was purchased.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the correct address of the B.S.A. Service Department?\", \n",
    "        \"answer\": \"B.S.A. MOTOR CYCLES LIMITED, SERVICE DEPARTMENT, ARMOURY ROAD, BIRMINGHAM 11\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the recommended procedure for claiming assistance under the guarantee for a new motorcycle?\", \n",
    "        \"answer\": \"The owner must do so through the dealer from whom the machine was purchased.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the recommended torque wrench setting for the Supreme model?\", \n",
    "        \"answer\": \"1 to 3\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# ========== CELL 5: Configure Your Model ==========\n",
    "# CUSTOMIZE THIS: Change to your model name\n",
    "adapter_name = \"Prithwiraj731/Gemma2-2b_Two-Wheeler\"\n",
    "base_model_name = \"google/gemma-2-2b\"\n",
    "\n",
    "# IMPORTANT: Choose which prompt format to use\n",
    "# Try different formats until you find what works for YOUR model\n",
    "PROMPT_FORMAT = \"simple\"  # Options: \"simple\", \"instruction\", \"gemma\", \"chat\"\n",
    "\n",
    "print(\"üîß Model Configuration:\")\n",
    "print(f\"   Adapter: {adapter_name}\")\n",
    "print(f\"   Base Model: {base_model_name}\")\n",
    "print(f\"   Prompt Format: {PROMPT_FORMAT}\")\n",
    "\n",
    "\n",
    "# ========== CELL 6: Load Model ==========\n",
    "print(\"\\nüì• Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_name)\n",
    "\n",
    "print(\"üì• Loading base model with 4-bit quantization...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"üì• Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_name)\n",
    "model.eval()\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\\n\")\n",
    "\n",
    "\n",
    "# ========== CELL 7: Define Answer Generation Function with Multiple Format Options ==========\n",
    "def generate_answer(question, max_new_tokens=100, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Generate answer with multiple prompt format options.\n",
    "    \n",
    "    Args:\n",
    "        question: The input question\n",
    "        max_new_tokens: Maximum tokens to generate (reduced to 100)\n",
    "        temperature: Generation randomness (0.1=very deterministic)\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Choose prompt format based on PROMPT_FORMAT setting\n",
    "    if PROMPT_FORMAT == \"simple\":\n",
    "        # Simple Q&A format (most common for fine-tuned models)\n",
    "        prompt = f\"{question}\\n\"\n",
    "    \n",
    "    elif PROMPT_FORMAT == \"instruction\":\n",
    "        # Instruction-style format\n",
    "        prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n",
    "    \n",
    "    elif PROMPT_FORMAT == \"gemma\":\n",
    "        # Gemma 2 Chat Template Format\n",
    "        prompt = f\"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    \n",
    "    elif PROMPT_FORMAT == \"chat\":\n",
    "        # Generic chat format\n",
    "        prompt = f\"User: {question}\\nAssistant:\"\n",
    "    \n",
    "    else:\n",
    "        # Default fallback\n",
    "        prompt = question\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=False,  # Greedy decoding for most factual answers\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2  # Prevent repetition\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract answer based on format\n",
    "    if PROMPT_FORMAT == \"simple\":\n",
    "        # Remove the prompt\n",
    "        answer = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    elif PROMPT_FORMAT == \"instruction\":\n",
    "        if \"### Answer:\" in generated_text:\n",
    "            answer = generated_text.split(\"### Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    elif PROMPT_FORMAT == \"gemma\":\n",
    "        if \"<start_of_turn>model\" in generated_text:\n",
    "            answer = generated_text.split(\"<start_of_turn>model\")[-1].strip()\n",
    "            if \"<end_of_turn>\" in answer:\n",
    "                answer = answer.split(\"<end_of_turn>\")[0].strip()\n",
    "        else:\n",
    "            answer = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    elif PROMPT_FORMAT == \"chat\":\n",
    "        if \"Assistant:\" in generated_text:\n",
    "            answer = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "        else:\n",
    "            answer = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    else:\n",
    "        answer = generated_text.strip()\n",
    "    \n",
    "    # Clean up common issues\n",
    "    answer = answer.split('\\n')[0].strip()  # Take only first line\n",
    "    \n",
    "    # Handle empty answers\n",
    "    if not answer or len(answer.strip()) == 0:\n",
    "        answer = \"No answer generated\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "# ========== CELL 8: Generate Predictions ==========\n",
    "print(\"=\" * 70)\n",
    "print(\"ü§ñ GENERATING ANSWERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "generation_times = []\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    question = item[\"question\"]\n",
    "    reference = item[\"answer\"]\n",
    "    \n",
    "    print(f\"\\nüìå Question {i+1}/{len(dataset)}\")\n",
    "    print(f\"Q: {question[:80]}...\")\n",
    "    \n",
    "    # Time the generation\n",
    "    start_time = time.time()\n",
    "    prediction = generate_answer(question, temperature=0.1)\n",
    "    gen_time = time.time() - start_time\n",
    "    generation_times.append(gen_time)\n",
    "    \n",
    "    print(f\"‚ú® Generated: {prediction[:100]}...\")\n",
    "    print(f\"üìñ Reference: {reference[:100]}...\")\n",
    "    print(f\"‚è±Ô∏è  Time: {gen_time:.2f}s\")\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    references.append(reference)\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Average generation time: {sum(generation_times)/len(generation_times):.2f}s\")\n",
    "\n",
    "\n",
    "# ========== CELL 9: Calculate BERT Score ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä CALCULATING BERT SCORES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "P, R, F1 = score(\n",
    "    predictions, \n",
    "    references, \n",
    "    lang=\"en\",\n",
    "    verbose=True,\n",
    "    rescale_with_baseline=True\n",
    ")\n",
    "\n",
    "\n",
    "# ========== CELL 10: Display Detailed Results ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìà BERT SCORE RESULTS (DETAILED)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Question {i+1}: {item['question'][:60]}...\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"Generated: {predictions[i][:120]}...\")\n",
    "    print(f\"Reference: {references[i][:120]}...\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"  üìä Precision: {P[i].item():.4f}\")\n",
    "    print(f\"  üìä Recall:    {R[i].item():.4f}\")\n",
    "    print(f\"  üìä F1 Score:  {F1[i].item():.4f}\")  # FIXED: Was 'F' now 'F1'\n",
    "\n",
    "\n",
    "# ========== CELL 11: Display Summary Statistics ==========\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ AVERAGE BERT SCORES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "avg_precision = P.mean().item()\n",
    "avg_recall = R.mean().item()\n",
    "avg_f1 = F1.mean().item()\n",
    "\n",
    "print(f\"\\n  üìä Average Precision: {avg_precision:.4f} ({avg_precision*100:.2f}%)\")\n",
    "print(f\"  üìä Average Recall:    {avg_recall:.4f} ({avg_recall*100:.2f}%)\")\n",
    "print(f\"  üìä Average F1 Score:  {avg_f1:.4f} ({avg_f1*100:.2f}%)\")\n",
    "\n",
    "# Score interpretation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìñ SCORE INTERPRETATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nBERT Score Range: -1.0 (worst) to 1.0 (best)\")\n",
    "print(\"\\nQuality Guide:\")\n",
    "print(\"  üü¢ 0.7 - 1.0  : Excellent semantic similarity\")\n",
    "print(\"  üü° 0.5 - 0.7  : Good similarity\")\n",
    "print(\"  üü† 0.3 - 0.5  : Moderate similarity\")\n",
    "print(\"  üî¥ 0.0 - 0.3  : Poor similarity\")\n",
    "print(\"  ‚ö´ < 0.0       : Very poor / opposing meaning\")\n",
    "\n",
    "if avg_f1 >= 0.7:\n",
    "    status = \"üü¢ EXCELLENT\"\n",
    "elif avg_f1 >= 0.5:\n",
    "    status = \"üü° GOOD\"\n",
    "elif avg_f1 >= 0.3:\n",
    "    status = \"üü† MODERATE\"\n",
    "else:\n",
    "    status = \"üî¥ NEEDS IMPROVEMENT\"\n",
    "\n",
    "print(f\"\\nYour Model Status: {status}\")\n",
    "\n",
    "\n",
    "# ========== CELL 12: Results DataFrame ==========\n",
    "results_df = pd.DataFrame({\n",
    "    'Question': [item['question'][:50] + '...' if len(item['question']) > 50 else item['question'] for item in dataset],\n",
    "    'Generated': [p[:50] + '...' if len(p) > 50 else p for p in predictions],\n",
    "    'Reference': [r[:50] + '...' if len(r) > 50 else r for r in references],\n",
    "    'Precision': [f\"{p.item():.4f}\" for p in P],\n",
    "    'Recall': [f\"{r.item():.4f}\" for r in R],\n",
    "    'F1': [f\"{f.item():.4f}\" for f in F1]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã RESULTS SUMMARY TABLE\")\n",
    "print(\"=\" * 70)\n",
    "display(results_df)\n",
    "\n",
    "\n",
    "# ========== CELL 13: Save Results (Optional) ==========\n",
    "# Uncomment to save and download results\n",
    "\n",
    "# results_df.to_csv('bert_score_results.csv', index=False)\n",
    "# print(\"\\n‚úÖ Results saved to 'bert_score_results.csv'\")\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('bert_score_results.csv')\n",
    "\n",
    "\n",
    "# ========== CELL 14: Test All Prompt Formats (DIAGNOSTIC) ==========\n",
    "# Run this cell to test which format works best for your model\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üß™ TESTING ALL PROMPT FORMATS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_question = dataset[0][\"question\"]\n",
    "formats = [\"simple\", \"instruction\", \"gemma\", \"chat\"]\n",
    "\n",
    "print(f\"\\nTest Question: {test_question}\\n\")\n",
    "\n",
    "for fmt in formats:\n",
    "    old_format = PROMPT_FORMAT\n",
    "    PROMPT_FORMAT = fmt\n",
    "    \n",
    "    answer = generate_answer(test_question, max_new_tokens=80, temperature=0.1)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Format: {fmt.upper()}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"Answer: {answer[:150]}\")\n",
    "    \n",
    "    PROMPT_FORMAT = old_format\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° RECOMMENDATION:\")\n",
    "print(\"Look at the outputs above and choose the format that gives actual\")\n",
    "print(\"answers (not repetitions). Then update PROMPT_FORMAT in Cell 5!\")\n",
    "print(\"=\" * 70)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
